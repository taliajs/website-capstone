---
title: "Beneath the Surface Level: Examining the Flaws in the Algorithms, Software, and Programs Around Us"
---

This chapter was all about examining power. One of the main ideas mentioned was the idea of the underlying biases and/or discrimination that exists in the algorithms and programs that are being used in today's world. The authors mentioned how there are "problems of gender and racial bias in our information systems". This topic reminded me of a project I did for my CSC 212 (Programming with Data Structures/Java) class. We wrote our final paper on analyzing biases in code. 

My group and I were looking at a program called **COMPAS** (Correctional Offender Management Profiling for Alternative Sanctions), which is a recidivism risk assessment program used in some U.S courts to assess the likelihood of a defendant becoming a recidivist (this term is used to label a criminal who continuously reoffends). This recidivism score has played in the role in the criminal justice system as some judges will take this score into account when determining sentencing. (A higher recidivism score may lead to a higher sentencing and/or harsher restrictions- no parole, etc.)

There have been concerns raised about this COMPAS algorithm, mainly over the presence of algorithm bias. Some of the main variables that are used in this algorithm include things like: Gang Membership, Parents Separated, Friends Arrested, Residential Stability, Neighborhood Crime, School Suspensions, Money, Boredom, Sadness, Anger, and Criminal Thinking. Additioanlly, zip code is a main influential factor considered in the algorithm, and "with the history of housing biases in the United States, we know that zip codes are easily synoymous to race, ethnicity and class" (this is a direct quote from our final paper). 

We looked at the raw data of all the offenders in Broward County, Florida where the COMPAS algorithm had been used on them between 2013 and 2014 to better understand the decisions that the COMPAS algorithm makes, and analyze whether there is presence of racial bias or disparity within the software algorithm itself. What we found is that the probability of having a high decile score (the recidivism score allocated to the offender, or the percentile of potential risk assessment) for an individual belonging to the African-American community was higher than an individual belonging to a Caucasian-American community. It's important to note that the racial makeup of Broward County at the time of this study/data collection was 70.57% White, 20.54% Black or African American, 0.24% Native American, 2.25% Asian, 0.06% Pacific Islander, 3.00% from other races, and that the African-American community was the biggest cultural group in the Broward County Jail. We used logistic regression models to analyze the relationship between race, gender, and recidivism scores, and to help calculate the probabilities to determine the extent of the racial bias in the COMPAS algorithm. This is what we found: 

- There is a 45% probability of being Caucasian, male and assigned a high recidivism score compared to a 69% probability of being African-American, male and having a higher recidivism score in Broward County Jail. 
- There is a 47% probability of being African-American, female and being assigned a high recidivism score, while there is a 39% probability of being Caucasian, female and being assigned a high recidivism score in Broward County Jail.
- Overall (not taking into account gender), there is a 69% probability of being African-American and being assigned a high recidivism score in Broward County Jail, whereas there is only a 43% probability of being Caucasian and being assigned a high recidivism score in Broward County Jail. 

These results show that having a higher recidivism score is impacted by the race and gender of the offender, and also show that race has an bigger influence on recidivism score than gender. These results led us to conclude that the probability of having a high recidivism score is higher for African Americans than for Caucasians. These findings showed us the need to analyze deeper the "racial biases" and the predictive accuracy of the COMPAS algorithm (which is discussed more in depth in the paper). 

Overall, there were ethnic dangers and biases in this COMPAS algorithm and can have a detrimental impact on people's lives. This algorithm I analyzed for a group project is probably just one of many algorithms that have some implicit biases (whether intentional or not). This project highlighted that while on the bigger scale, these algorithms may be efficient, but if you dive deeper and take a closer look, it may be flawed and have serious consequences and repercussions. 

Something that is also interesting to note was our ability to obtain the raw data. This dataset and study on the COMPAS algorithm was made available by ProPublica (an independent non-profit newsroom that produces investigative journalism in the public interest). If this organization hadn't published the data and the study, my group and I wouldn't have been able to do this project. We had such a hard time finding the raw data itself. In order to examine the "power" and analyze the algorithms, programs and software around us, we need to obtain the data (which is another barrier/obstacle in itself). To examine the power, we need access: access to the data, and access to papers about the program, etc. Once we have access, we can start examining. 